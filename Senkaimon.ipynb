{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.schema import Document\n",
    "import os\n",
    "from IPython.display import display, Markdown\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set environment variable for protobuf\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF loaded successfully: fortnite.pdf\n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "local_path = \"fortnite.pdf\"\n",
    "if local_path:\n",
    "    pdf_text = extract_text_from_pdf(local_path)\n",
    "    print(f\"PDF loaded successfully: {local_path}\")\n",
    "else:\n",
    "    print(\"Upload a PDF file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spliting text into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text split into 77 chunks\n"
     ]
    }
   ],
   "source": [
    "# Split text into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_text(pdf_text)\n",
    "\n",
    "# Wrap each chunk into a Document object\n",
    "documents = [Document(page_content=chunk) for chunk in chunks]\n",
    "print(f\"Text split into {len(documents)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating vectorstore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector database created successfully\n"
     ]
    }
   ],
   "source": [
    "# vector database\n",
    "vector_db = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=OllamaEmbeddings(model=\"nomic-embed-text\"),\n",
    "    collection_name=\"local-rag\"\n",
    ")\n",
    "print(\"Vector database created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM and Retrieval Set up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up LLM and retrieval\n",
    "local_model = \"llama3.2\" \n",
    "llm = ChatOllama(model=local_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query prompt template\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to generate 4\n",
    "    different versions of the given user question to retrieve relevant documents from\n",
    "    a vector database. By generating multiple perspectives on the user question, your\n",
    "    goal is to help the user overcome some of the limitations of the distance-based\n",
    "    similarity search. Provide these alternative questions separated by newlines.\n",
    "    Original question: {question}\"\"\",\n",
    ")\n",
    "\n",
    "# Set up retriever\n",
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    vector_db.as_retriever(),\n",
    "    llm,\n",
    "    prompt=QUERY_PROMPT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG prompt template\n",
    "template = \"\"\"Answer the question based ONLY on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chain\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat with PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def chat_with_pdf(question):\n",
    "    \"\"\"\n",
    "    Chat with the PDF using the RAG chain.\n",
    "    \"\"\"\n",
    "    return display(Markdown(chain.invoke(question)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here are some of the various terms and conditions mentioned in the provided text:\n",
       "\n",
       "1. **Arbitration**: The agreement includes a provision for arbitration, which requires disputes to be resolved through private arbitration rather than in court.\n",
       "2. **Confidential Information**: Confidential information is defined as non-public information related to the Software, including gameplay, content, Game Currency, and Services.\n",
       "3. **Cheat Detection and Cheats**: Cheat detection refers to functionality intended to identify and prevent cheating, while cheats refer to programs or methods that provide an unfair competitive advantage in the Software.\n",
       "4. **Defenses against Unenforceable Provisions**: If any provision of this Agreement is held by a court or tribunal of competent jurisdiction to be unenforceable, that provision shall be enforced only to the furthest extent possible under applicable law.\n",
       "5. **Dispute Resolution**: The agreement specifies which court has jurisdiction over disputes between Epic and the user (Superior Court of Wake County, North Carolina).\n",
       "6. **Exclusive Remedy**: This Agreement does not confer any rights or remedies on any person other than the parties to this Agreement, except as expressly stated.\n",
       "7. **Governing Law**: The agreement is governed by the laws of the State of North Carolina.\n",
       "8. **Opt-out Notice**: Users have 30 days to provide an opt-out notice if they do not wish to be bound by arbitration provisions.\n",
       "9. **Platform-Specific Terms**: The agreement includes platform-specific terms, such as those applicable to Sony PlayStation users (Section 18.1).\n",
       "10. **Right of Assignment**: Epic may at any time assign, transfer, charge, or subcontract all or any of its rights or obligations under this Agreement.\n",
       "11. **Termination**: Sections 2, 5-13, and 15-17 will survive any termination of this Agreement.\n",
       "12. **Transfer Restrictions**: If restrictions on the transfer of the Software in this Agreement are not enforceable under the law of your country, then this Agreement will be binding on any recipient of the Software.\n",
       "\n",
       "These terms and conditions cover various aspects of the relationship between Epic and its users, including arbitration, confidentiality, cheat detection, dispute resolution, governing law, platform-specific terms, and termination."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example 1\n",
    "chat_with_pdf(\"What are the various terms and conditions?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up the vector database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector database deleted successfully\n"
     ]
    }
   ],
   "source": [
    "# Optional: Clean up when done \n",
    "vector_db.delete_collection()\n",
    "print(\"Vector database deleted successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
